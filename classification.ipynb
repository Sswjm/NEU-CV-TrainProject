{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "author: Mu Yibo \n",
        "institution: Northeastern University\n",
        "date: 2022/5\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-qfuafAuiyMN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4ee8c8e2-e9c2-4905-8a24-26733c67bb9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nauthor: Mu Yibo \\ninstitution: Northeastern University\\ndate: 2022/5\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data\n",
        "!mkdir data\n",
        "!unzip -d /content/data /content/drive/MyDrive/data/aptos2019-blindness-detection.zip"
      ],
      "metadata": {
        "id": "wiBZ6NHAPIBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "MrRR75HRkelx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c50bfe2-c716-4402-82ce-a183df377926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "# 导入相关库\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, datasets, transforms\n",
        "from torchvision.io import read_image\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import timm\n",
        "from timm.data.transforms_factory import create_transform"
      ],
      "metadata": {
        "id": "ed9np3OM1T9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置相关参数\n",
        "batch_size = 16\n",
        "epochs = 30\n",
        "lr = 5e-5\n",
        "\n",
        "num_classes = 4\n",
        "image_size = 380\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "labels_map = {\n",
        "    0: \"No DR\",\n",
        "    1: \"Mild\",\n",
        "    2: \"Moderate\",\n",
        "    3: \"Severe\",\n",
        "#    4: \"Proliferative DR\",\n",
        "}\n",
        "coef = [0.5, 1.5, 2.5, 3.5]"
      ],
      "metadata": {
        "id": "Fl5FK2PreQT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_class(value):\n",
        "  pre = 0\n",
        "  if value < coef[0]:\n",
        "    pre = 0.0\n",
        "  elif value >= coef[0] and value < coef[1]:\n",
        "    pre = 1.0\n",
        "  elif value >= coef[1] and value < coef[2]:\n",
        "    pre = 2.0\n",
        "  else:\n",
        "    pre = 3.0\n",
        "\n",
        "  return pre"
      ],
      "metadata": {
        "id": "dw3Rqc4pg5Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.导入数据集并进行图像预处理"
      ],
      "metadata": {
        "id": "xye-BFz51pkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## code from https://www.kaggle.com/code/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy?scriptVersionId=20340219\n",
        "# 图像预处理函数\n",
        "def crop_image1(img,tol=7):\n",
        "    # img is image data\n",
        "    # tol  is tolerance\n",
        "        \n",
        "    mask = img>tol\n",
        "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
        "\n",
        "def crop_image_from_gray(img,tol=7):\n",
        "    if img.ndim ==2:\n",
        "        mask = img>tol\n",
        "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
        "    elif img.ndim==3:\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        mask = gray_img>tol\n",
        "        \n",
        "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
        "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
        "            return img # return original image\n",
        "        else:\n",
        "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
        "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
        "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
        "    #         print(img1.shape,img2.shape,img3.shape)\n",
        "            img = np.stack([img1,img2,img3],axis=-1)\n",
        "    #         print(img.shape)\n",
        "        return img"
      ],
      "metadata": {
        "id": "_IgYwTFghOVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用上面的函数进行图像预处理\n",
        "# 仅当需要输出预处理后图片到文件夹中时才使用此单元格\n",
        "input_path = 'data'\n",
        "train_path = input_path + '/train_images'\n",
        "test_path = input_path + '/test_images'\n",
        "\n",
        "train_output_path = input_path + '/preprocessed_train_images'\n",
        "test_output_path = input_path + '/preprocessed_test_images'\n",
        "\n",
        "try:\n",
        "    os.mkdir(train_output_path)\n",
        "    os.mkdir(test_output_path)\n",
        "except:\n",
        "    print(\"Dictionary already exists\")\n",
        "\n",
        "# 处理训练集数据\n",
        "for image_path in tqdm(os.listdir(train_path)):\n",
        "    input_image = os.path.join(train_path, image_path)\n",
        "\n",
        "    image = cv2.imread(input_image)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = crop_image_from_gray(image)\n",
        "    image = cv2.resize(image, (image_size, image_size))\n",
        "    image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)\n",
        "\n",
        "    output_image = os.path.join(train_output_path, image_path)\n",
        "    cv2.imwrite(output_image, image)\n",
        "\n",
        "# 处理测试集数据\n",
        "for image_path in tqdm(os.listdir(test_path)):\n",
        "    input_image = os.path.join(test_path, image_path)\n",
        "\n",
        "    image = cv2.imread(input_image)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = crop_image_from_gray(image)\n",
        "    image = cv2.resize(image, (image_size, image_size))\n",
        "    image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)\n",
        "\n",
        "    output_image = os.path.join(test_output_path, image_path)\n",
        "    cv2.imwrite(output_image, image)\n"
      ],
      "metadata": {
        "id": "hlTIzdZMiA-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6414c3c6-7754-4cca-a3b5-2b7bc2bb0e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary already exists\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3662/3662 [11:31<00:00,  5.30it/s]\n",
            "100%|██████████| 1928/1928 [03:20<00:00,  9.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.a 使用数字标签，转换为torch.tensor，此时num_classes == 1"
      ],
      "metadata": {
        "id": "9-mrINGLmDhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置path参数，导入csv标签，划分训练集和验证集\n",
        "input_path = 'data'\n",
        "train_path = input_path + '/train_images'\n",
        "test_path = input_path + '/test_images'\n",
        "\n",
        "train_output_path = input_path + '/preprocessed_train_images'\n",
        "test_output_path = input_path + '/preprocessed_test_images'\n",
        "\n",
        "# 读取csv annotation file\n",
        "df_train = pd.read_csv(input_path + '/train.csv')\n",
        "df_test = pd.read_csv(input_path + '/test.csv')\n",
        "\n",
        "# drop label == 4   295 images\n",
        "df_train.drop(df_train[df_train.diagnosis == 4].index, inplace=True) \n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "#df_train.hist()\n",
        "#x = df_train['id_code']      #images\n",
        "#y = df_train['diagnosis']    #labels\n",
        "\n",
        "train_list, valid_list = train_test_split(df_train, test_size=0.1, random_state=42)\n",
        "test_list = df_test\n",
        "print(len(df_train))\n",
        "print(len(train_list))\n",
        "print(len(valid_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgx6gz0dMq_A",
        "outputId": "4fd573a8-38d0-4104-d035-0d3250e9b6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3367\n",
            "3030\n",
            "337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_list.hist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "6m0nwJGnJdo8",
        "outputId": "09aea842-debf-4f66-f7e4-2c2fe2237784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f27cb485a90>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class APTOSDataset(Dataset):\n",
        "#     def __init__(self, annotations_dataframe, img_dir, transform=None, target_transform=None):\n",
        "#         self.img_labels = annotations_dataframe\n",
        "#         self.img_dir = img_dir\n",
        "#         self.transform = transform\n",
        "#         self.target_transform = target_transform\n",
        "    \n",
        "#     def __len__(self):\n",
        "#         return len(self.img_labels)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "#         img_path = img_path + '.png'\n",
        "        \n",
        "\n",
        "#         image = cv2.imread(img_path)\n",
        "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "#         image = crop_image_from_gray(image)\n",
        "#         image = cv2.resize(image, (image_size, image_size))\n",
        "#         image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)\n",
        "\n",
        "#         image = transforms.ToPILImage()(image)\n",
        "        \n",
        "#         label = torch.tensor(self.img_labels.iloc[idx, 1])\n",
        "\n",
        "#         if self.transform:\n",
        "#             image = self.transform(image)\n",
        "#         if self.target_transform:\n",
        "#             label = self.target_transform(label)\n",
        "\n",
        "#         return image, label\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, annotations_dataframe, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = annotations_dataframe\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        img_path = img_path + '.png'\n",
        "        #image = read_image(img_path)\n",
        "        image = Image.open(img_path)\n",
        "        #label = self.img_labels.iloc[idx, 1]\n",
        "        label = torch.tensor(self.img_labels.iloc[idx, 1])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "NYRtlsymMta4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义transforms\n",
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomResizedCrop(image_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "DvnK3WCkMv_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms_timm = create_transform(image_size, is_training=True, auto_augment='rand-m9-mstd0.5')\n",
        "train_transforms_timm"
      ],
      "metadata": {
        "id": "k_17_hJMius7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b10e185-07a5-4969-8a1c-b2280fcbbcdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    RandomResizedCropAndInterpolation(size=(380, 380), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
              "    RandomHorizontalFlip(p=0.5)\n",
              "    RandAugment(n=2, ops=\n",
              "\tAugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=Posterize, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=Solarize, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=Color, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=Contrast, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=Brightness, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=Sharpness, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))\n",
              "    ToTensor()\n",
              "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'data/preprocessed_train_images'\n",
        "test_dir = 'data/preprocessed_test_images'\n",
        "\n",
        "train_dataset = APTOSDataset(train_list, train_dir, transform=train_transforms_timm)\n",
        "valid_dataset = APTOSDataset(valid_list, train_dir, transform=train_transforms_timm)\n",
        "test_dataset = APTOSDataset(test_list, test_dir, transform=test_transforms)\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader  = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(len(train_dataset), len(train_dataloader))\n",
        "print(len(valid_dataset), len(valid_dataloader))\n",
        "print(len(test_dataset), len(test_dataloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8rVQniEMyLN",
        "outputId": "f68db75a-fea9-4f84-b771-ec64caa93f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3030 48\n",
            "337 6\n",
            "1928 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.b 使用扩展的one-hot编码作为标签"
      ],
      "metadata": {
        "id": "vtGbozcYlzco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 4  #reset num_classes because of the change of labels"
      ],
      "metadata": {
        "id": "XRN-CpX1m7v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置path参数，导入csv标签，划分训练集和验证集\n",
        "input_path = 'data'\n",
        "train_path = input_path + '/train_images'\n",
        "test_path = input_path + '/test_images'\n",
        "\n",
        "train_output_path = input_path + '/preprocessed_train_images'\n",
        "test_output_path = input_path + '/preprocessed_test_images'\n",
        "\n",
        "# 读取csv annotation file\n",
        "df_train = pd.read_csv(input_path + '/train.csv')\n",
        "df_test = pd.read_csv(input_path + '/test.csv')\n",
        "\n",
        "# drop label == 4   295 images\n",
        "df_train.drop(df_train[df_train.diagnosis == 4].index, inplace=True) \n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "#df_train.hist()\n",
        "#x = df_train['id_code']      #images\n",
        "#y = df_train['diagnosis']    #labels\n",
        "\n",
        "train_list, valid_list = train_test_split(df_train, test_size=0.1, random_state=42)\n",
        "test_list = df_test\n",
        "print(len(df_train))\n",
        "print(len(train_list))\n",
        "print(len(valid_list))\n",
        "# print(len(valid_list))"
      ],
      "metadata": {
        "id": "SHuCHflsl9Vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9584a290-97d2-44d6-df81-d19a163b8af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3367\n",
            "3030\n",
            "337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 重载Dataset类\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, annotations_dataframe, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = annotations_dataframe\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        img_path = img_path + '.png'\n",
        "        #image = read_image(img_path)\n",
        "        image = Image.open(img_path)\n",
        "        #label = self.img_labels.iloc[idx, 1]\n",
        "        label_ori = self.img_labels.iloc[idx, 1]\n",
        "        if label_ori == 0:\n",
        "          label = torch.tensor([1, 0, 0, 0])\n",
        "        elif label_ori == 1:\n",
        "          label = torch.tensor([1, 1, 0, 0])\n",
        "        elif label_ori == 2:\n",
        "          label = torch.tensor([1, 1, 1, 0])\n",
        "        else:\n",
        "          label = torch.tensor([1, 1, 1, 1])\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "GNqvjTSHnydt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_transforms_timm = create_transform(image_size, is_training=True, auto_augment='rand-m9-mstd0.5')\n",
        "target_transform = transforms.Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
      ],
      "metadata": {
        "id": "8PBQHdVRZQOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'data/preprocessed_train_images'\n",
        "test_dir = 'data/preprocessed_test_images'\n",
        "\n",
        "train_dataset = APTOSDataset(train_list, train_dir, transform=train_transforms_timm)\n",
        "valid_dataset = APTOSDataset(valid_list, train_dir, transform=train_transforms_timm)\n",
        "test_dataset = APTOSDataset(test_list, test_dir, transform=test_transforms)\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader  = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(len(train_dataset), len(train_dataloader))\n",
        "print(len(valid_dataset), len(valid_dataloader))\n",
        "print(len(test_dataset), len(test_dataloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04vMAH4sZUE3",
        "outputId": "9903c536-729d-427b-82ed-c18e9a533a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3030 95\n",
            "337 11\n",
            "1928 61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.c 使用int型的数字标签，跟MNIST数据集的标签类型相同"
      ],
      "metadata": {
        "id": "ROMT1rIarlI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置path参数，导入csv标签，划分训练集和验证集\n",
        "input_path = 'data'\n",
        "train_path = input_path + '/train_images'\n",
        "test_path = input_path + '/test_images'\n",
        "\n",
        "train_output_path = input_path + '/preprocessed_train_images'\n",
        "test_output_path = input_path + '/preprocessed_test_images'\n",
        "\n",
        "# 读取csv annotation file\n",
        "df_train = pd.read_csv(input_path + '/train.csv')\n",
        "df_test = pd.read_csv(input_path + '/test.csv')\n",
        "\n",
        "# drop label == 4   295 images\n",
        "df_train.drop(df_train[df_train.diagnosis == 4].index, inplace=True) \n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "#df_train.hist()\n",
        "#x = df_train['id_code']      #images\n",
        "#y = df_train['diagnosis']    #labels\n",
        "\n",
        "train_list, valid_list = train_test_split(df_train, test_size=0.1, random_state=42)\n",
        "test_list = df_test\n",
        "print(len(df_train))\n",
        "print(len(train_list))\n",
        "print(len(valid_list))"
      ],
      "metadata": {
        "id": "N2pMMh6Cr6we",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bae16b3-68cd-49b3-f133-80af45c8ed8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3367\n",
            "3030\n",
            "337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, annotations_dataframe, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = annotations_dataframe\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        img_path = img_path + '.png'\n",
        "        #image = read_image(img_path)\n",
        "        image = Image.open(img_path)\n",
        "        #label = self.img_labels.iloc[idx, 1]\n",
        "        label = int(self.img_labels.iloc[idx, 1])  \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "IE5PbAgksDzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义transforms\n",
        "# train_transforms = transforms.Compose(\n",
        "#     [\n",
        "#         transforms.RandomResizedCrop(image_size),\n",
        "#         transforms.RandomHorizontalFlip(),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "test_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_transforms_timm = create_transform(image_size, is_training=True, auto_augment='rand-m9-mstd0.5')\n",
        "target_transform = transforms.Lambda(lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
      ],
      "metadata": {
        "id": "Q83F2duMsIWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'data/train_images'\n",
        "test_dir = 'data/test_images'\n",
        "\n",
        "train_dataset = APTOSDataset(train_list, train_dir, transform=train_transforms_timm)\n",
        "valid_dataset = APTOSDataset(valid_list, train_dir, transform=train_transforms_timm)\n",
        "test_dataset = APTOSDataset(test_list, test_dir, transform=test_transforms)\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader  = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(len(train_dataset), len(train_dataloader))\n",
        "print(len(valid_dataset), len(valid_dataloader))\n",
        "print(len(test_dataset), len(test_dataloader))"
      ],
      "metadata": {
        "id": "Wd39Xz_TsOs_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da337aee-b933-4d41-899a-029d9fa77f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3030 190\n",
            "337 22\n",
            "1928 121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.a 模型训练"
      ],
      "metadata": {
        "id": "2o56bkM5M2Zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.efficientnet_b5(pretrained=False)\n",
        "#print(model)\n",
        "in_features = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(in_features=in_features, out_features=num_classes)\n",
        "\n",
        "model.load_state_dict(torch.load('effi_b5_224x224.pth'))\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "N4USQlTnM6DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "scheduler = StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "save_path = 'efficientnet_b4_b.pth'\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(epochs):\n",
        "    # train\n",
        "    print('Learning Rate: {}'.format(scheduler.get_last_lr()))\n",
        "    model.train()\n",
        "    train_loss = .0\n",
        "    for i, (images, labels) in enumerate(tqdm(train_dataloader)):\n",
        "        images = images.to(device, dtype=torch.float)\n",
        "        labels = labels.to(device, dtype=torch.float)\n",
        "        print(labels.shape)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "    print ('Epoch [{}/{}], Train Loss: {:.4f}' .format(epoch+1, epochs, train_loss/len(train_dataloader)))\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = .0\n",
        "    with torch.no_grad():\n",
        "      for i, (images, labels) in enumerate(valid_dataloader):\n",
        "        images = images.to(device, dtype=torch.float)\n",
        "        labels = labels.to(device, dtype=torch.float)\n",
        "\n",
        "        outputs = model(image)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        val_loss += loss.item()\n",
        "\n",
        "    print('Val Loss: {:.4f}'.format(val_loss/len(valid_dataloader)))\n",
        "\n",
        "    # validate the model, show the loss and accuarcy of the validation set\n",
        "    # model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "    # avg_val_loss = .0\n",
        "    # with torch.no_grad():\n",
        "    #     correct = 0\n",
        "    #     total = 0\n",
        "    #     for i, (images, labels) in enumerate(valid_dataloader):\n",
        "    #         labels = labels.view(-1, 1)\n",
        "    #         images = images.to(device, dtype=torch.float)\n",
        "    #         labels = labels.to(device, dtype=torch.float)\n",
        "    #         outputs = model(images)\n",
        "            \n",
        "    #         val_loss = criterion(outputs, labels)\n",
        "    #         avg_val_loss += val_loss.item() / len(valid_dataloader)\n",
        "\n",
        "    #         outputs = outputs.view(-1)\n",
        "    #         labels = labels.view(-1)\n",
        "    #         predicts = []\n",
        "    #         for idx in outputs:\n",
        "    #           predicts.append(predict_class(idx))\n",
        "    #         predicts = torch.tensor(predicts).to(device)\n",
        "    \n",
        "    #         total += labels.size(0)\n",
        "    #         correct += (predicts == labels).sum().item()\n",
        "\n",
        "    # print('Val Loss: {:.4f}, Val Accuarcy: {:.4f} %'.format(avg_val_loss, 100*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "# Save the model checkpoint\n",
        "# torch.save(model.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "NUAgOor_M9Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.c 模型训练"
      ],
      "metadata": {
        "id": "n00X2YAnr06J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.efficientnet_b4(pretrained=True)\n",
        "\n",
        "in_features = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(in_features=in_features, out_features=num_classes)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "XL-8aReW3Azq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "#optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-5)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "scheduler = StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "save_path = 'effi_b4_300x300.pth'\n",
        "\n",
        "epochs_train_loss = []\n",
        "epochs_val_loss = []\n",
        "epochs_train_acc = []\n",
        "epochs_val_acc = []\n",
        "\n",
        "best_loss = 100.0\n",
        "best_acc = 0\n",
        "# Train the model\n",
        "total_step = len(train_dataloader)\n",
        "for epoch in range(epochs):\n",
        "    # train\n",
        "    print('Learning Rate: {}'.format(scheduler.get_last_lr()))\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(tqdm(train_dataloader)):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print('Epoch [{}/{}]' .format(epoch+1, epochs))\n",
        "    print('Train loss: {:.4f}'.format(train_loss / len(train_dataloader)))\n",
        "    epochs_train_loss.append(train_loss / len(train_dataloader))\n",
        "    print('Train Accuracy of the model on the validation images: {} %'.format(100 * correct / total))\n",
        "    epochs_train_acc.append(100 * correct / total)\n",
        "\n",
        "    # validate the model\n",
        "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in valid_dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Val loss: {:.4f}'.format(val_loss / len(valid_dataloader)))\n",
        "    epochs_val_loss.append(val_loss / len(valid_dataloader))\n",
        "    print('Val Accuracy of the model on the validation images: {} %'.format(100 * correct / total))\n",
        "    epochs_val_acc.append(100 * correct / total)\n",
        "\n",
        "    if (val_loss / len(valid_dataloader)) < best_loss:\n",
        "      best_loss = val_loss / len(valid_dataloader)\n",
        "    if (100 * correct / total) > best_acc:\n",
        "      best_acc = 100 * correct / total\n",
        "      print('Save best model,loss: {:.4f}  acc: {}'.format(best_loss, best_acc))\n",
        "      torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "# Save the model checkpoint"
      ],
      "metadata": {
        "id": "uV-iX8Evr3at",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d721878d-5482-4b69-be10-5d1d7a24384a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate: [5e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:01<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30]\n",
            "Train loss: 1.1527\n",
            "Train Accuracy of the model on the validation images: 54.224422442244226 %\n",
            "Val loss: 0.8983\n",
            "Val Accuracy of the model on the validation images: 62.31454005934718 %\n",
            "Save best model,loss: 0.8983  acc: 62.31454005934718\n",
            "Learning Rate: [5e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:02<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/30]\n",
            "Train loss: 0.7377\n",
            "Train Accuracy of the model on the validation images: 74.91749174917491 %\n",
            "Val loss: 0.5597\n",
            "Val Accuracy of the model on the validation images: 78.63501483679525 %\n",
            "Save best model,loss: 0.5597  acc: 78.63501483679525\n",
            "Learning Rate: [5e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:00<00:00,  2.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/30]\n",
            "Train loss: 0.5831\n",
            "Train Accuracy of the model on the validation images: 78.58085808580859 %\n",
            "Val loss: 0.4647\n",
            "Val Accuracy of the model on the validation images: 83.0860534124629 %\n",
            "Save best model,loss: 0.4647  acc: 83.0860534124629\n",
            "Learning Rate: [2.5e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:00<00:00,  2.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/30]\n",
            "Train loss: 0.5352\n",
            "Train Accuracy of the model on the validation images: 80.26402640264027 %\n",
            "Val loss: 0.4763\n",
            "Val Accuracy of the model on the validation images: 83.67952522255193 %\n",
            "Save best model,loss: 0.4647  acc: 83.67952522255193\n",
            "Learning Rate: [2.5e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:00<00:00,  2.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/30]\n",
            "Train loss: 0.5146\n",
            "Train Accuracy of the model on the validation images: 80.56105610561056 %\n",
            "Val loss: 0.5023\n",
            "Val Accuracy of the model on the validation images: 82.7893175074184 %\n",
            "Learning Rate: [2.5e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:03<00:00,  2.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/30]\n",
            "Train loss: 0.4923\n",
            "Train Accuracy of the model on the validation images: 80.6930693069307 %\n",
            "Val loss: 0.4567\n",
            "Val Accuracy of the model on the validation images: 83.38278931750742 %\n",
            "Learning Rate: [1.25e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:02<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/30]\n",
            "Train loss: 0.4709\n",
            "Train Accuracy of the model on the validation images: 81.74917491749174 %\n",
            "Val loss: 0.4723\n",
            "Val Accuracy of the model on the validation images: 83.97626112759644 %\n",
            "Save best model,loss: 0.4567  acc: 83.97626112759644\n",
            "Learning Rate: [1.25e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:01<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/30]\n",
            "Train loss: 0.4844\n",
            "Train Accuracy of the model on the validation images: 81.32013201320132 %\n",
            "Val loss: 0.4136\n",
            "Val Accuracy of the model on the validation images: 85.7566765578635 %\n",
            "Save best model,loss: 0.4136  acc: 85.7566765578635\n",
            "Learning Rate: [1.25e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:01<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/30]\n",
            "Train loss: 0.4696\n",
            "Train Accuracy of the model on the validation images: 81.74917491749174 %\n",
            "Val loss: 0.4593\n",
            "Val Accuracy of the model on the validation images: 83.0860534124629 %\n",
            "Learning Rate: [6.25e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:01<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/30]\n",
            "Train loss: 0.4824\n",
            "Train Accuracy of the model on the validation images: 81.45214521452145 %\n",
            "Val loss: 0.5445\n",
            "Val Accuracy of the model on the validation images: 83.38278931750742 %\n",
            "Learning Rate: [6.25e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:00<00:00,  2.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/30]\n",
            "Train loss: 0.4643\n",
            "Train Accuracy of the model on the validation images: 81.91419141914191 %\n",
            "Val loss: 0.4396\n",
            "Val Accuracy of the model on the validation images: 85.16320474777449 %\n",
            "Learning Rate: [6.25e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:01<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/30]\n",
            "Train loss: 0.4787\n",
            "Train Accuracy of the model on the validation images: 81.51815181518151 %\n",
            "Val loss: 0.4517\n",
            "Val Accuracy of the model on the validation images: 82.49258160237389 %\n",
            "Learning Rate: [3.125e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:02<00:00,  2.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/30]\n",
            "Train loss: 0.4477\n",
            "Train Accuracy of the model on the validation images: 82.67326732673267 %\n",
            "Val loss: 0.4577\n",
            "Val Accuracy of the model on the validation images: 83.97626112759644 %\n",
            "Learning Rate: [3.125e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:02<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/30]\n",
            "Train loss: 0.4615\n",
            "Train Accuracy of the model on the validation images: 82.17821782178218 %\n",
            "Val loss: 0.4445\n",
            "Val Accuracy of the model on the validation images: 84.27299703264094 %\n",
            "Learning Rate: [3.125e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:01<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/30]\n",
            "Train loss: 0.4786\n",
            "Train Accuracy of the model on the validation images: 81.71617161716172 %\n",
            "Val loss: 0.4135\n",
            "Val Accuracy of the model on the validation images: 86.05341246290801 %\n",
            "Save best model,loss: 0.4135  acc: 86.05341246290801\n",
            "Learning Rate: [1.5625e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:03<00:00,  2.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/30]\n",
            "Train loss: 0.4585\n",
            "Train Accuracy of the model on the validation images: 81.78217821782178 %\n",
            "Val loss: 0.4025\n",
            "Val Accuracy of the model on the validation images: 84.86646884272997 %\n",
            "Learning Rate: [1.5625e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:07<00:00,  2.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/30]\n",
            "Train loss: 0.4624\n",
            "Train Accuracy of the model on the validation images: 81.84818481848185 %\n",
            "Val loss: 0.4403\n",
            "Val Accuracy of the model on the validation images: 85.16320474777449 %\n",
            "Learning Rate: [1.5625e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:02<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/30]\n",
            "Train loss: 0.4633\n",
            "Train Accuracy of the model on the validation images: 82.11221122112211 %\n",
            "Val loss: 0.4264\n",
            "Val Accuracy of the model on the validation images: 83.67952522255193 %\n",
            "Learning Rate: [7.8125e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:01<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/30]\n",
            "Train loss: 0.4624\n",
            "Train Accuracy of the model on the validation images: 81.78217821782178 %\n",
            "Val loss: 0.4610\n",
            "Val Accuracy of the model on the validation images: 83.67952522255193 %\n",
            "Learning Rate: [7.8125e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:02<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/30]\n",
            "Train loss: 0.4632\n",
            "Train Accuracy of the model on the validation images: 82.11221122112211 %\n",
            "Val loss: 0.4317\n",
            "Val Accuracy of the model on the validation images: 84.27299703264094 %\n",
            "Learning Rate: [7.8125e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:01<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/30]\n",
            "Train loss: 0.4561\n",
            "Train Accuracy of the model on the validation images: 82.07920792079207 %\n",
            "Val loss: 0.4330\n",
            "Val Accuracy of the model on the validation images: 84.27299703264094 %\n",
            "Learning Rate: [3.90625e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:04<00:00,  2.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/30]\n",
            "Train loss: 0.4700\n",
            "Train Accuracy of the model on the validation images: 81.84818481848185 %\n",
            "Val loss: 0.4203\n",
            "Val Accuracy of the model on the validation images: 85.7566765578635 %\n",
            "Learning Rate: [3.90625e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:03<00:00,  2.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/30]\n",
            "Train loss: 0.4405\n",
            "Train Accuracy of the model on the validation images: 83.06930693069307 %\n",
            "Val loss: 0.4004\n",
            "Val Accuracy of the model on the validation images: 85.16320474777449 %\n",
            "Learning Rate: [3.90625e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:01<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/30]\n",
            "Train loss: 0.4537\n",
            "Train Accuracy of the model on the validation images: 81.71617161716172 %\n",
            "Val loss: 0.4187\n",
            "Val Accuracy of the model on the validation images: 84.86646884272997 %\n",
            "Learning Rate: [1.953125e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:01<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/30]\n",
            "Train loss: 0.4560\n",
            "Train Accuracy of the model on the validation images: 82.01320132013201 %\n",
            "Val loss: 0.4278\n",
            "Val Accuracy of the model on the validation images: 83.97626112759644 %\n",
            "Learning Rate: [1.953125e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:01<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/30]\n",
            "Train loss: 0.4607\n",
            "Train Accuracy of the model on the validation images: 81.68316831683168 %\n",
            "Val loss: 0.4073\n",
            "Val Accuracy of the model on the validation images: 85.45994065281899 %\n",
            "Learning Rate: [1.953125e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:00<00:00,  2.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/30]\n",
            "Train loss: 0.4563\n",
            "Train Accuracy of the model on the validation images: 82.11221122112211 %\n",
            "Val loss: 0.4144\n",
            "Val Accuracy of the model on the validation images: 84.86646884272997 %\n",
            "Learning Rate: [9.765625e-08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:01<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/30]\n",
            "Train loss: 0.4533\n",
            "Train Accuracy of the model on the validation images: 82.40924092409242 %\n",
            "Val loss: 0.6855\n",
            "Val Accuracy of the model on the validation images: 84.27299703264094 %\n",
            "Learning Rate: [9.765625e-08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:00<00:00,  2.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/30]\n",
            "Train loss: 0.4594\n",
            "Train Accuracy of the model on the validation images: 82.40924092409242 %\n",
            "Val loss: 0.4223\n",
            "Val Accuracy of the model on the validation images: 85.7566765578635 %\n",
            "Learning Rate: [9.765625e-08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [07:00<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/30]\n",
            "Train loss: 0.4661\n",
            "Train Accuracy of the model on the validation images: 82.01320132013201 %\n",
            "Val loss: 0.3823\n",
            "Val Accuracy of the model on the validation images: 86.35014836795253 %\n",
            "Save best model,loss: 0.3823  acc: 86.35014836795253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.推理"
      ],
      "metadata": {
        "id": "H7-7eDGzs68m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp effi_b4_300x300.pth drive/MyDrive/data"
      ],
      "metadata": {
        "id": "MGJRwGD5u3Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coef = [0.5, 1.5, 2.5, 3.5]\n",
        "def predict_class(value):\n",
        "  pre = 0\n",
        "  if value < coef[0]:\n",
        "    pre = 0.0\n",
        "  elif value >= coef[0] and value < coef[1]:\n",
        "    pre = 1.0\n",
        "  elif value >= coef[1] and value < coef[2]:\n",
        "    pre = 2.0\n",
        "  else:\n",
        "    pre = 3.0\n",
        "\n",
        "  return pre"
      ],
      "metadata": {
        "id": "FvLIsDFfVkfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 针对整型标签进行推理\n",
        "# model = models.efficientnet_b4()\n",
        "# in_features = model.classifier[1].in_features\n",
        "# model.classifier[1] = nn.Linear(in_features=in_features, out_features=num_classes)\n",
        "\n",
        "# model.load_state_dict(torch.load('efficientnet_b4.pth'))\n",
        "# model.to(device)\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for i, (images, labels) in enumerate(valid_dataloader):\n",
        "    #labels = labels.view(-1, 1)\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = model(images)\n",
        "    print(outputs.shape)\n",
        "    print(labels.shape)\n",
        "    #labels = labels.view(-1)\n",
        "#     predicts = []\n",
        "#     for idx in outputs:\n",
        "#       predicts.append(predict_class(idx))\n",
        "#     predicts = torch.tensor(predicts).to(device)\n",
        "    \n",
        "#     total += labels.size(0)\n",
        "#     correct += (predicts == labels).sum().item()\n",
        "\n",
        "# print('final accuarcy : {:.4f} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "id": "aqB_1H8lVeCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.efficientnet_b5()\n",
        "in_features = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(in_features=in_features, out_features=num_classes)\n",
        "\n",
        "model.load_state_dict(torch.load('effi_b5_224x224.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for i, (images, labels) in enumerate(valid_dataloader):\n",
        "    #labels = labels.view(-1, 1)\n",
        "    images = images.to(device, dtype=torch.float)\n",
        "    labels = labels.to(device, dtype=torch.float)\n",
        "\n",
        "    outputs = model(images).view(-1)\n",
        "    #labels = labels.view(-1)\n",
        "    predicts = []\n",
        "    for idx in outputs:\n",
        "      predicts.append(predict_class(idx))\n",
        "    predicts = torch.tensor(predicts).to(device)\n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicts == labels).sum().item()\n",
        "\n",
        "print('final accuarcy : {:.4f} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmRKpT35-bMT",
        "outputId": "f9dfe0b1-c297-4c2b-9eef-3c3ccbd16107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final accuarcy : 83.0861 %\n"
          ]
        }
      ]
    }
  ]
}